name: Data Pipeline CI

# 1. The Trigger
# Run on every push or pull request to the main branch
on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

# 2. The Environment
jobs:
  build-and-test:
    runs-on: ubuntu-latest

    steps:
    # A. Get the Code
    - name: Checkout Code
      uses: actions/checkout@v4

    # B. Setup Python
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    # C. Install Dependencies
    - name: Install Libraries
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pre-commit

    # D. Create Dummy Environment (CRITICAL)
    # We mock the .env file and keys so Python imports don't crash
    - name: Create Mock Environment
      run: |
        touch .env
        echo "GCP_PROJECT_ID=ci_test_project" >> .env
        echo "DBT_KEY_PATH=/tmp/dummy_key.json" >> .env
        echo "BQ_DATASET_ID=ci_test_dataset" >> .env
        echo "BQ_TABLE_ID=ci_test_table" >> .env
        echo "CSV_PATH=data/raw/creditcard.csv" >> .env
        echo "DBT_PROJECT_DIR=./dbt_project" >> .env
        echo "DAGSTER_POSTGRES_HOST=localhost" >> .env
        echo "POSTGRES_USER=test_user" >> .env
        echo "POSTGRES_PASSWORD=test_pass" >> .env
        echo "POSTGRES_DB=test_db" >> .env
        # Create a dummy JSON key file so dbt/python doesn't crash looking for it
        echo '{"type": "service_account"}' > /tmp/dummy_key.json

    # E. Run Linter (Quality Check)
    # Checks code formatting using the rules in .pre-commit-config.yaml
    - name: Run Linter (pre-commit)
      run: |
        pre-commit run --all-files

    # F. Run Unit Tests (Logic Check)
    # Verifies that Python assets and definitions import correctly
    - name: Run Unit Tests
      run: |
        pytest dagster_project/tests/

    # G. Validate dbt Models (Syntax Check)
    # We create a temporary, fake profile so dbt can parse the SQL without needing BigQuery
    - name: Validate dbt Models
      run: |
        # 1. Create the dbt directory in the home folder
        mkdir -p ~/.dbt

        # 2. Write a temporary profiles.yml with hardcoded fake credentials
        cat <<EOF > ~/.dbt/profiles.yml
        credit_card_analytics:
          target: dev
          outputs:
            dev:
              type: bigquery
              method: service-account
              project: ci_dummy_project
              dataset: ci_dummy_dataset
              threads: 1
              keyfile: /tmp/dummy_key.json
              location: US
              priority: interactive
        EOF

        # 3. Run the checks
        cd dbt_project
        dbt deps
        dbt parse
